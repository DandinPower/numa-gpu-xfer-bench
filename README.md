# numa-gpu-xfer-bench

This NUMA-aware benchmark provides latency and bandwidth measurements for CPU-GPU data transfers using libnuma for interleaved allocation, cudaMemcpyAsync for async operations, and concurrent multi-GPU support via command-line options.

## Prerequisites

To build and run this benchmark, ensure the following requirements are met:

- **System**: A machine with multiple NUMA nodes and multiple NVIDIA GPUs.
- **CUDA Toolkit**: Installed and configured (e.g., `/usr/local/cuda`). Ensure `LD_LIBRARY_PATH` includes `/usr/local/cuda/lib64`.
- **libnuma**: Development package installed for NUMA-aware memory allocation.

On Ubuntu, install libnuma with:

```bash
sudo apt-get install libnuma-dev
```

Verify CUDA installation by checking `nvcc --version`. If not installed, download and install it from the [NVIDIA CUDA Toolkit page](https://developer.nvidia.com/cuda-toolkit).

## Building

Clone the repository and build the benchmark:

```bash
git clone <repository-url>
cd numa-gpu-xfer-bench
make
```

This generates the `benchmark` executable in the `/build` directory.

## Usage

Run the benchmark with the following command-line arguments:

```bash
./build/benchmark --ngpus <int> --benchmark_bytes <size_t> --iterations_per_bench <int> --operation_type <string> --numa_nodes <string> --pin_memory <0 or 1>
```

### Arguments
- `--ngpus`: Number of GPUs to use (e.g., `2`).
- `--benchmark_bytes`: Size of data to transfer in bytes (e.g., `1048576` for 1 MB).
- `--iterations_per_bench`: Number of iterations per benchmark (e.g., `100`).
- `--operation_type`: Operations to perform (`R` for host-to-device, `W` for device-to-host, `R,W` for both).
- `--numa_nodes`: Comma-separated NUMA nodes for interleaved allocation (e.g., `"0,1"`).
- `--pin_memory`: Pin CPU memory for CUDA (`0` to disable, `1` to enable).

### Example
```bash
./build/benchmark --ngpus 2 --benchmark_bytes 1048576 --iterations_per_bench 100 --operation_type "R,W" --numa_nodes "0,1" --pin_memory 1
```

This runs the benchmark on 2 GPUs, transferring 1 MB of data 100 times, performing both read and write operations, using NUMA nodes 0 and 1, with pinned memory.

## Output
The program outputs average latency (in ms) and bandwidth (in GB/s) for host-to-device and/or device-to-host transfers, depending on the `operation_type`.

### Benchmark Scripts

The repository includes a Bash script, `benchmark_scripts.sh`, to automate running the benchmark across various configurations and generate CSV-formatted results. This script iterates over combinations of GPU counts, NUMA node configurations, and data transfer sizes, capturing latency and bandwidth metrics for both host-to-device (`R`) and device-to-host (`W`) operations.

To execute the script and save the output to a CSV file, use the following command:

```bash
bash benchmark_scripts.sh > example/one_8dimm_cxl_aic.csv
```

This runs the benchmark and redirects the output to `example/one_8dimm_cxl_aic.csv`, which matches the sample output provided in the repository. The script assumes the `benchmark` executable has been built (via `make`) and is located in the `build/` directory.

#### Script Details
- **Configurations Tested**:
  - Number of GPUs (`ngpus`): `1`, `2`
  - NUMA node combinations (`numa_nodes`): `"0"`, `"0,1"`, `"3"`, `"0,3"` (where node 3 is a CXL memory node in the example system)
  - Data transfer sizes (`benchmark_bytes`): Ranging from 1 KB (`1024`) to 2 GB (`2147483648`) in powers of 2
- **Fixed Parameters**:
  - Iterations per benchmark: `100`
  - Operation type: `"R,W"` (both read and write)
  - Memory pinning: Enabled (`--pin_memory 1`)
- **Output Format**:
  The script generates a CSV with the following columns:
  - `ngpus`: Number of GPUs used
  - `numa_nodes`: Comma-separated list of NUMA nodes
  - `access_size(bytes)`: Size of data transferred
  - `read_latency(ms)`: Average host-to-device latency
  - `read_bw(gib/s)`: Average host-to-device bandwidth
  - `write_latency(ms)`: Average device-to-host latency
  - `write_bw(gib/s)`: Average device-to-host bandwidth

#### Customization
You can modify `benchmark_scripts.sh` to adjust the tested configurations:
- Edit `ngpus_items` to change the number of GPUs.
- Update `numa_nodes_items` to reflect your system's NUMA topology (use `numactl --hardware` to verify available nodes).
- Adjust `benchmark_bytes_items` to test different transfer sizes.

#### Example Output
The file `example/one_8dimm_cxl_aic.csv` provides a sample output generated by the script on a system with CXL memory (node 3). It includes results for 1 and 2 GPUs across various NUMA node combinations and transfer sizes, demonstrating the benchmark's ability to measure performance differences.

## Notes
- Ensure the number of GPUs specified not exceeded the system's available GPUs.
- When the number of GPUs is greater than two, remember that they will execute benchmarks simultaneously and asynchronously, causing the operations to overlap.
- NUMA node indices should be valid for your system (check with `numactl --hardware`).
- Pinning memory (`--pin_memory 1`) may improve performance but increases setup overhead.